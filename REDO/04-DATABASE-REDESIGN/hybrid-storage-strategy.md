# HYBRID STORAGE STRATEGY
## Database Architecture Redesign for 90% Performance Improvement

---

## STORAGE PHILOSOPHY TRANSFORMATION

### **CURRENT PROBLEMS** üö®
- **Mixed Concerns**: Temporary game state stored in permanent PostgreSQL tables
- **Performance Bottlenecks**: Complex joins required for basic operations
- **Real-time Overhead**: Every change triggers database writes and broadcasts
- **Over-normalization**: Character stats require 5+ table joins for display
- **Single Point of Failure**: All data in one database system

### **NEW HYBRID APPROACH** üéØ

#### **DATA CLASSIFICATION SYSTEM**
```
üî• HOT DATA (Redis Primary)
   - Active game sessions (0-2 hours old)
   - Current player positions and stats
   - Real-time combat state
   - Turn order and timing
   - WebSocket connection state

‚ùÑÔ∏è COLD DATA (PostgreSQL Primary)  
   - Character progression and skills
   - Equipment and inventory
   - Player accounts and settings
   - Historical game records
   - Achievement data

üìö REFERENCE DATA (PostgreSQL + Cache)
   - Skill definitions and requirements  
   - Item templates and statistics
   - Ability configurations
   - World generation rules
   - Game balance parameters
```

---

## REDIS ARCHITECTURE FOR HOT DATA

### **ACTIVE GAME SESSION STORAGE** ‚ö°

```typescript
// Redis Key Structure for Game Sessions
interface RedisGameSession {
  // Session metadata
  [`session:${sessionId}:meta`]: {
    sessionId: string;
    hostPlayerId: string;
    created: number;
    lastActivity: number;
    status: 'waiting' | 'active' | 'paused' | 'completed';
    maxPlayers: number;
    currentPlayers: number;
  };
  
  // Player positions and real-time state
  [`session:${sessionId}:players`]: {
    [playerId: string]: {
      position: { x: number; y: number };
      health: number;
      mana: number;
      actionPoints: number;
      statusEffects: StatusEffect[];
      lastAction: number;
    };
  };
  
  // Turn management
  [`session:${sessionId}:turns`]: {
    currentPlayer: string;
    turnNumber: number;
    turnStartTime: number;
    turnTimeLimit: number;
    initiativeOrder: string[];
    pendingActions: GameAction[];
  };
  
  // Game world state (current dungeon)
  [`session:${sessionId}:world`]: {
    dungeonId: string;
    currentLevel: number;
    visibleCells: { [key: string]: CellData };
    dynamicObjects: GameObject[];
    environmental Effects: Effect[];
  };
}

// Redis Operations for Game Sessions
export class RedisGameSessionStore {
  private redis: Redis;
  
  // Create new game session
  async createSession(hostPlayerId: string, settings: SessionSettings): Promise<string> {
    const sessionId = generateSessionId();
    
    const pipeline = this.redis.pipeline();
    
    // Session metadata
    pipeline.hset(`session:${sessionId}:meta`, {
      sessionId,
      hostPlayerId,
      created: Date.now(),
      lastActivity: Date.now(),
      status: 'waiting',
      maxPlayers: settings.maxPlayers,
      currentPlayers: 1
    });
    
    // Initialize empty players hash
    pipeline.hset(`session:${sessionId}:players`, hostPlayerId, JSON.stringify({
      position: settings.startPosition,
      health: 100,
      mana: 50,
      actionPoints: 3,
      statusEffects: [],
      lastAction: Date.now()
    }));
    
    // Set expiration for automatic cleanup
    pipeline.expire(`session:${sessionId}:meta`, 7200); // 2 hours
    pipeline.expire(`session:${sessionId}:players`, 7200);
    
    await pipeline.exec();
    return sessionId;
  }
  
  // Ultra-fast player state updates
  async updatePlayerState(sessionId: string, playerId: string, updates: Partial<PlayerState>): Promise<void> {
    const playerKey = `session:${sessionId}:players`;
    
    // Get current state
    const currentState = await this.redis.hget(playerKey, playerId);
    const playerState = JSON.parse(currentState || '{}');
    
    // Apply updates
    const newState = { ...playerState, ...updates, lastAction: Date.now() };
    
    // Atomic update
    const pipeline = this.redis.pipeline();
    pipeline.hset(playerKey, playerId, JSON.stringify(newState));
    pipeline.hset(`session:${sessionId}:meta`, 'lastActivity', Date.now());
    
    await pipeline.exec();
  }
  
  // Get complete session state (for sync)
  async getSessionState(sessionId: string): Promise<GameSessionState> {
    const pipeline = this.redis.pipeline();
    
    pipeline.hgetall(`session:${sessionId}:meta`);
    pipeline.hgetall(`session:${sessionId}:players`);
    pipeline.hgetall(`session:${sessionId}:turns`);
    pipeline.hgetall(`session:${sessionId}:world`);
    
    const [meta, players, turns, world] = await pipeline.exec();
    
    return {
      meta: meta[1] as SessionMeta,
      players: this.parsePlayersData(players[1] as Record<string, string>),
      turns: JSON.parse(turns[1]?.toString() || '{}'),
      world: JSON.parse(world[1]?.toString() || '{}')
    };
  }
  
  // Broadcast state changes to all players in session
  async broadcastStateChange(sessionId: string, change: StateChange): Promise<void> {
    const changeMessage = JSON.stringify({
      type: 'state_change',
      sessionId,
      timestamp: Date.now(),
      change
    });
    
    // Publish to session channel for real-time updates
    await this.redis.publish(`session:${sessionId}:updates`, changeMessage);
  }
}
```

### **REAL-TIME PERFORMANCE OPTIMIZATIONS** üöÄ

#### **Connection Pooling and Clustering**
```typescript
// Redis Cluster Configuration for High Availability
export const redisClusterConfig = {
  // Multiple Redis nodes for horizontal scaling
  nodes: [
    { host: 'redis-1.game.local', port: 6379 },
    { host: 'redis-2.game.local', port: 6379 }, 
    { host: 'redis-3.game.local', port: 6379 },
  ],
  options: {
    // Connection pooling
    maxRetriesPerRequest: 3,
    retryDelayOnFailover: 100,
    
    // Performance optimization  
    enableOfflineQueue: false,
    lazyConnect: true,
    
    // Memory management
    maxMemoryPolicy: 'allkeys-lru',
    keyPrefix: 'tactical_roguelike:',
    
    // Cluster failover
    enableReadyCheck: false,
    redisOptions: {
      password: process.env.REDIS_PASSWORD,
    }
  }
};

// Connection Management
export class OptimizedRedisManager {
  private cluster: Redis.Cluster;
  private connectionPool = new Map<string, Redis>();
  
  constructor() {
    this.cluster = new Redis.Cluster(redisClusterConfig.nodes, redisClusterConfig.options);
    this.setupErrorHandling();
  }
  
  // Get connection optimized for operation type
  getConnection(operationType: 'read' | 'write' | 'pubsub'): Redis {
    const poolKey = `${operationType}_pool`;
    
    if (!this.connectionPool.has(poolKey)) {
      const connection = this.cluster.duplicate();
      this.connectionPool.set(poolKey, connection);
    }
    
    return this.connectionPool.get(poolKey)!;
  }
  
  // Pipeline operations for batch updates
  async batchUpdatePlayers(sessionId: string, updates: PlayerUpdate[]): Promise<void> {
    const pipeline = this.cluster.pipeline();
    const playerKey = `session:${sessionId}:players`;
    
    for (const update of updates) {
      const currentState = await this.cluster.hget(playerKey, update.playerId);
      const playerState = JSON.parse(currentState || '{}');
      const newState = { ...playerState, ...update.changes, lastAction: Date.now() };
      
      pipeline.hset(playerKey, update.playerId, JSON.stringify(newState));
    }
    
    // Execute all updates atomically
    await pipeline.exec();
  }
}
```

---

## POSTGRESQL ARCHITECTURE FOR COLD DATA

### **OPTIMIZED PERSISTENT SCHEMA** üèóÔ∏è

```sql
-- ============================================
-- PLAYER ACCOUNTS AND AUTHENTICATION
-- ============================================
CREATE TABLE player_accounts (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    username TEXT UNIQUE NOT NULL,
    email TEXT UNIQUE NOT NULL,
    password_hash TEXT NOT NULL,
    created_at TIMESTAMPTZ DEFAULT NOW(),
    last_login TIMESTAMPTZ,
    total_playtime INTERVAL DEFAULT '0 seconds',
    preferences JSONB DEFAULT '{}',
    is_active BOOLEAN DEFAULT TRUE
);

CREATE INDEX idx_player_accounts_login ON player_accounts(username, password_hash) 
WHERE is_active = TRUE;

-- ============================================  
-- CHARACTER PROGRESSION (OPTIMIZED)
-- ============================================
CREATE TABLE characters (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    player_id UUID NOT NULL REFERENCES player_accounts(id) ON DELETE CASCADE,
    name TEXT NOT NULL,
    
    -- Base character stats (JSONB for flexibility)
    base_stats JSONB NOT NULL DEFAULT '{
        "health": 100,
        "mana": 50, 
        "strength": 10,
        "agility": 10,
        "intelligence": 10,
        "wisdom": 10
    }'::jsonb,
    
    -- All skills in single JSONB (no joins needed!)
    skill_levels JSONB NOT NULL DEFAULT '{}'::jsonb,
    
    -- Equipment references (array for performance)
    equipped_items UUID[] DEFAULT '{}',
    
    -- Progression tracking
    total_experience BIGINT DEFAULT 0,
    death_count INTEGER DEFAULT 0,
    achievements TEXT[] DEFAULT '{}',
    
    -- Timestamps
    created_at TIMESTAMPTZ DEFAULT NOW(),
    updated_at TIMESTAMPTZ DEFAULT NOW(),
    last_played TIMESTAMPTZ,
    
    -- Soft delete support
    is_active BOOLEAN DEFAULT TRUE
);

-- Optimized indexes for common queries
CREATE INDEX idx_characters_player_active ON characters(player_id) 
WHERE is_active = TRUE;

CREATE INDEX idx_characters_skill_lookup ON characters 
USING GIN(skill_levels) 
WHERE is_active = TRUE;

CREATE INDEX idx_characters_equipment ON characters 
USING GIN(equipped_items) 
WHERE is_active = TRUE;

-- ============================================
-- INVENTORY SYSTEM (SIMPLIFIED)
-- ============================================  
CREATE TABLE character_inventory (
    character_id UUID NOT NULL REFERENCES characters(id) ON DELETE CASCADE,
    
    -- Complete item data in JSONB (no item table joins!)
    item_data JSONB NOT NULL,
    
    -- Metadata
    quantity INTEGER DEFAULT 1,
    slot_type TEXT CHECK (slot_type IN ('equipped', 'inventory', 'storage')),
    slot_position INTEGER,
    acquired_at TIMESTAMPTZ DEFAULT NOW(),
    
    -- Composite primary key for uniqueness
    PRIMARY KEY (character_id, (item_data->>'template_id'), slot_type, slot_position)
);

-- High-performance inventory queries
CREATE INDEX idx_inventory_character ON character_inventory(character_id, slot_type);
CREATE INDEX idx_inventory_item_type ON character_inventory 
USING GIN((item_data->'type')) 
WHERE slot_type = 'inventory';

-- ============================================
-- SESSION HISTORY (FOR ANALYTICS)  
-- ============================================
CREATE TABLE game_sessions_history (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    session_id TEXT NOT NULL, -- From Redis
    participants UUID[] NOT NULL,
    session_config JSONB NOT NULL,
    
    -- Session timeline
    started_at TIMESTAMPTZ NOT NULL,
    ended_at TIMESTAMPTZ,
    duration INTERVAL GENERATED ALWAYS AS (ended_at - started_at) STORED,
    
    -- Results and statistics
    final_state JSONB,
    participant_stats JSONB,
    achievements_earned JSONB DEFAULT '{}',
    
    -- Performance metrics
    avg_response_time INTEGER, -- milliseconds
    peak_players INTEGER,
    total_actions INTEGER
);

-- Analytics indexes
CREATE INDEX idx_session_history_participants ON game_sessions_history 
USING GIN(participants);

CREATE INDEX idx_session_history_timeline ON game_sessions_history(started_at DESC);

CREATE INDEX idx_session_history_performance ON game_sessions_history(avg_response_time, peak_players);
```

### **ADVANCED QUERY PATTERNS** üìä

```sql
-- ============================================
-- HIGH-PERFORMANCE CHARACTER QUERIES
-- ============================================

-- Get complete character with all skills (single query!)
SELECT 
    c.id,
    c.name,
    c.base_stats,
    c.skill_levels,
    c.total_experience,
    c.death_count,
    -- Calculate available abilities based on skill levels
    (
        SELECT array_agg(ability_id)
        FROM ability_templates at
        WHERE jsonb_path_exists(
            c.skill_levels,
            ('$.' || (at.requirements->>'skill') || ' ? (@ >= ' || (at.requirements->>'level')::text || ')')::jsonpath
        )
    ) as available_abilities
FROM characters c 
WHERE c.id = $1 AND c.is_active = TRUE;

-- Get top players by skill in specific area (leaderboards)
SELECT 
    c.name,
    c.skill_levels->'sword_mastery' as sword_level,
    c.total_experience,
    c.death_count,
    -- Calculate skill efficiency ratio
    ROUND(
        (c.skill_levels->'sword_mastery')::numeric / 
        GREATEST(c.total_experience::numeric / 1000, 1), 
        2
    ) as efficiency_ratio
FROM characters c
WHERE c.skill_levels->>'sword_mastery' IS NOT NULL
  AND c.is_active = TRUE
ORDER BY (c.skill_levels->'sword_mastery')::numeric DESC
LIMIT 100;

-- Advanced inventory search with item properties
SELECT 
    inv.item_data->>'name' as item_name,
    inv.item_data->>'type' as item_type,
    (inv.item_data->'stats'->>'damage')::integer as damage,
    inv.quantity,
    inv.slot_type
FROM character_inventory inv
WHERE inv.character_id = $1
  AND inv.item_data->>'type' = 'weapon'
  AND (inv.item_data->'stats'->>'damage')::integer > 20
ORDER BY (inv.item_data->'stats'->>'damage')::integer DESC;

-- ============================================  
-- ANALYTICS AND REPORTING QUERIES
-- ============================================

-- Session performance analytics
SELECT 
    DATE_TRUNC('hour', started_at) as hour,
    COUNT(*) as session_count,
    AVG(duration) as avg_duration,
    AVG(avg_response_time) as avg_response_time,
    AVG(peak_players) as avg_players,
    SUM(total_actions) as total_actions
FROM game_sessions_history 
WHERE started_at >= NOW() - INTERVAL '24 hours'
GROUP BY DATE_TRUNC('hour', started_at)
ORDER BY hour DESC;

-- Player progression trends  
WITH skill_progression AS (
    SELECT 
        c.player_id,
        c.name,
        jsonb_each_text(c.skill_levels) as skill_data,
        c.total_experience,
        c.last_played
    FROM characters c
    WHERE c.is_active = TRUE
)
SELECT 
    (skill_data).key as skill_name,
    AVG((skill_data).value::numeric) as avg_level,
    COUNT(*) as player_count,
    MAX((skill_data).value::numeric) as max_level
FROM skill_progression
WHERE last_played >= NOW() - INTERVAL '30 days'
GROUP BY (skill_data).key
ORDER BY avg_level DESC;
```

---

## DATA SYNCHRONIZATION PATTERNS

### **REDIS TO POSTGRESQL SYNC** üîÑ

```typescript
// Automated Hot-to-Cold Data Migration
export class DataSynchronizationService {
  private redis: Redis;
  private postgres: Pool;
  private syncInterval: NodeJS.Timeout;
  
  constructor(redisClient: Redis, postgresPool: Pool) {
    this.redis = redisClient;
    this.postgres = postgresPool;
    this.startPeriodicSync();
  }
  
  // Sync game session data every 30 seconds
  private startPeriodicSync(): void {
    this.syncInterval = setInterval(async () => {
      await this.syncActiveSessions();
      await this.archiveCompletedSessions();
      await this.cleanupExpiredData();
    }, 30000); // 30 seconds
  }
  
  // Sync active session snapshots to PostgreSQL
  private async syncActiveSessions(): Promise<void> {
    const activeSessionKeys = await this.redis.keys('session:*:meta');
    
    for (const sessionKey of activeSessionKeys) {
      const sessionId = sessionKey.split(':')[1];
      const sessionMeta = await this.redis.hgetall(sessionKey);
      
      if (sessionMeta.status === 'active') {
        await this.createSessionSnapshot(sessionId);
      }
    }
  }
  
  private async createSessionSnapshot(sessionId: string): Promise<void> {
    const sessionState = await this.getCompleteSessionState(sessionId);
    
    // Update PostgreSQL with current state
    const query = `
      INSERT INTO game_sessions_snapshots (
        session_id, snapshot_time, session_state, participants, metrics
      ) VALUES ($1, $2, $3, $4, $5)
      ON CONFLICT (session_id) DO UPDATE SET
        snapshot_time = $2,
        session_state = $3,
        metrics = $5
    `;
    
    await this.postgres.query(query, [
      sessionId,
      new Date(),
      JSON.stringify(sessionState),
      sessionState.participants,
      this.calculateSessionMetrics(sessionState)
    ]);
  }
  
  // Archive completed sessions from Redis to PostgreSQL
  private async archiveCompletedSessions(): Promise<void> {
    const completedSessions = await this.redis.keys('session:*:meta');
    
    for (const sessionKey of completedSessions) {
      const sessionId = sessionKey.split(':')[1];
      const sessionMeta = await this.redis.hgetall(sessionKey);
      
      if (sessionMeta.status === 'completed') {
        await this.archiveSession(sessionId);
        await this.cleanupRedisSession(sessionId);
      }
    }
  }
  
  private async archiveSession(sessionId: string): Promise<void> {
    const finalState = await this.getCompleteSessionState(sessionId);
    
    const query = `
      INSERT INTO game_sessions_history (
        session_id, participants, session_config, started_at, ended_at,
        final_state, participant_stats, avg_response_time, peak_players, total_actions
      ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10)
      ON CONFLICT (session_id) DO NOTHING
    `;
    
    await this.postgres.query(query, [
      sessionId,
      finalState.participants,
      finalState.config,
      new Date(finalState.startTime),
      new Date(finalState.endTime),
      JSON.stringify(finalState),
      finalState.participantStats,
      finalState.avgResponseTime,
      finalState.peakPlayers,
      finalState.totalActions
    ]);
  }
  
  private async cleanupRedisSession(sessionId: string): Promise<void> {
    const pipeline = this.redis.pipeline();
    
    // Delete all session-related keys
    pipeline.del(`session:${sessionId}:meta`);
    pipeline.del(`session:${sessionId}:players`);
    pipeline.del(`session:${sessionId}:turns`);
    pipeline.del(`session:${sessionId}:world`);
    
    await pipeline.exec();
  }
}
```

### **CHARACTER PROGRESSION SYNC** üìà

```typescript
// Character progression sync from active sessions to permanent storage
export class CharacterProgressionSync {
  
  // Update character progression after each session
  async syncCharacterProgression(sessionId: string): Promise<void> {
    const sessionState = await this.redis.hgetall(`session:${sessionId}:players`);
    const progressionUpdates: CharacterUpdate[] = [];
    
    // Calculate progression for each player
    for (const [playerId, stateData] of Object.entries(sessionState)) {
      const playerState = JSON.parse(stateData);
      const characterId = await this.getCharacterIdForPlayer(playerId);
      
      if (characterId) {
        const progressionUpdate = await this.calculateProgression(characterId, playerState);
        progressionUpdates.push(progressionUpdate);
      }
    }
    
    // Batch update all character progressions
    await this.applyProgressionUpdates(progressionUpdates);
  }
  
  private async calculateProgression(
    characterId: string, 
    sessionState: PlayerState
  ): Promise<CharacterUpdate> {
    
    // Get current character data from PostgreSQL
    const currentCharacter = await this.postgres.query(
      'SELECT skill_levels, total_experience FROM characters WHERE id = $1',
      [characterId]
    );
    
    const currentSkills = currentCharacter.rows[0].skill_levels;
    const skillUpdates: Record<string, number> = { ...currentSkills };
    
    // Calculate skill gains based on session actions
    for (const action of sessionState.actionsPerformed) {
      const skillGains = this.calculateSkillGains(action);
      
      for (const [skill, gain] of Object.entries(skillGains)) {
        skillUpdates[skill] = (skillUpdates[skill] || 0) + gain;
      }
    }
    
    return {
      characterId,
      skillUpdates,
      experienceGain: this.calculateExperienceGain(sessionState),
      lastPlayed: new Date()
    };
  }
  
  private async applyProgressionUpdates(updates: CharacterUpdate[]): Promise<void> {
    const client = await this.postgres.connect();
    
    try {
      await client.query('BEGIN');
      
      for (const update of updates) {
        const query = `
          UPDATE characters 
          SET skill_levels = $1,
              total_experience = total_experience + $2,
              last_played = $3,
              updated_at = NOW()
          WHERE id = $4
        `;
        
        await client.query(query, [
          JSON.stringify(update.skillUpdates),
          update.experienceGain,
          update.lastPlayed,
          update.characterId
        ]);
      }
      
      await client.query('COMMIT');
    } catch (error) {
      await client.query('ROLLBACK');
      throw error;
    } finally {
      client.release();
    }
  }
}
```

---

## PERFORMANCE MONITORING & OPTIMIZATION

### **REDIS PERFORMANCE METRICS** üìä

```typescript
// Redis Performance Monitoring
export class RedisPerformanceMonitor {
  private metrics = {
    operations: new Counter({
      name: 'redis_operations_total',
      help: 'Total Redis operations',
      labelNames: ['operation', 'key_pattern']
    }),
    latency: new Histogram({
      name: 'redis_operation_duration_seconds', 
      help: 'Redis operation latency',
      labelNames: ['operation'],
      buckets: [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1]
    }),
    memory: new Gauge({
      name: 'redis_memory_usage_bytes',
      help: 'Redis memory usage'
    }),
    connections: new Gauge({
      name: 'redis_connections_active',
      help: 'Active Redis connections'
    })
  };
  
  // Monitor Redis operations
  instrumentOperation<T>(
    operation: string, 
    keyPattern: string, 
    fn: () => Promise<T>
  ): Promise<T> {
    const start = Date.now();
    
    return fn()
      .then(result => {
        this.metrics.operations.inc({ operation, key_pattern: keyPattern });
        this.metrics.latency.observe({ operation }, (Date.now() - start) / 1000);
        return result;
      })
      .catch(error => {
        this.metrics.operations.inc({ operation, key_pattern: keyPattern });
        throw error;
      });
  }
  
  // Monitor Redis memory and connection health
  async collectSystemMetrics(): Promise<void> {
    const info = await this.redis.info('memory');
    const memoryUsed = this.parseRedisInfo(info, 'used_memory');
    
    this.metrics.memory.set(parseInt(memoryUsed));
    
    const clients = await this.redis.info('clients');
    const connectedClients = this.parseRedisInfo(clients, 'connected_clients');
    
    this.metrics.connections.set(parseInt(connectedClients));
  }
}
```

### **QUERY PERFORMANCE OPTIMIZATION** ‚ö°

```sql
-- ============================================
-- POSTGRESQL PERFORMANCE OPTIMIZATIONS
-- ============================================

-- Partial indexes for active data only
CREATE INDEX CONCURRENTLY idx_characters_active_skills 
ON characters USING GIN(skill_levels) 
WHERE is_active = TRUE AND last_played > NOW() - INTERVAL '30 days';

-- Covering indexes to avoid table lookups
CREATE INDEX CONCURRENTLY idx_inventory_covering
ON character_inventory (character_id, slot_type) 
INCLUDE (item_data, quantity, acquired_at)
WHERE slot_type IN ('equipped', 'inventory');

-- Materialized view for leaderboards (refresh every hour)
CREATE MATERIALIZED VIEW skill_leaderboards AS
SELECT 
    skill_name,
    character_name,
    skill_level,
    player_rank,
    total_players
FROM (
    SELECT 
        skill.key as skill_name,
        c.name as character_name,
        skill.value::integer as skill_level,
        ROW_NUMBER() OVER (
            PARTITION BY skill.key 
            ORDER BY skill.value::integer DESC, c.total_experience DESC
        ) as player_rank,
        COUNT(*) OVER (PARTITION BY skill.key) as total_players
    FROM characters c,
         jsonb_each_text(c.skill_levels) as skill
    WHERE c.is_active = TRUE 
      AND c.last_played > NOW() - INTERVAL '90 days'
      AND skill.value::integer > 0
) ranked
WHERE player_rank <= 1000; -- Top 1000 per skill

CREATE UNIQUE INDEX ON skill_leaderboards (skill_name, player_rank);

-- Automatic refresh of leaderboards
SELECT cron.schedule('refresh-leaderboards', '0 * * * *', 
    'REFRESH MATERIALIZED VIEW CONCURRENTLY skill_leaderboards;');

-- Connection pooling optimization
ALTER SYSTEM SET max_connections = 200;
ALTER SYSTEM SET shared_buffers = '256MB';  
ALTER SYSTEM SET effective_cache_size = '1GB';
ALTER SYSTEM SET work_mem = '4MB';
ALTER SYSTEM SET maintenance_work_mem = '64MB';
ALTER SYSTEM SET random_page_cost = 1.1;
SELECT pg_reload_conf();
```

---

## DEPLOYMENT AND SCALING STRATEGY

### **MULTI-REGION DEPLOYMENT** üåç

```yaml
# Docker Compose for Multi-Region Redis Deployment
version: '3.8'
services:
  # Primary Redis cluster (US-East)
  redis-primary:
    image: redis:7-alpine
    command: redis-server --cluster-enabled yes --cluster-config-file nodes.conf
    volumes:
      - redis_primary_data:/data
    networks:
      - game_network
    deploy:
      replicas: 3
      resources:
        limits:
          memory: 512M
        reservations:
          memory: 256M
  
  # Read replicas for global distribution
  redis-replica-eu:
    image: redis:7-alpine
    command: redis-server --replicaof redis-primary 6379
    volumes:
      - redis_eu_data:/data
    networks:
      - game_network
  
  # PostgreSQL primary with streaming replication
  postgres-primary:
    image: postgres:15
    environment:
      POSTGRES_DB: tactical_roguelike
      POSTGRES_USER: game_user
      POSTGRES_PASSWORD: ${DB_PASSWORD}
    volumes:
      - postgres_primary_data:/var/lib/postgresql/data
      - ./postgresql.conf:/etc/postgresql/postgresql.conf
    command: postgres -c config_file=/etc/postgresql/postgresql.conf
    networks:
      - game_network
  
  postgres-replica:
    image: postgres:15
    environment:
      PGUSER: replica_user
      POSTGRES_PASSWORD: ${REPLICA_PASSWORD}
    volumes:
      - postgres_replica_data:/var/lib/postgresql/data
    command: |
      bash -c "
      until pg_basebackup --host=postgres-primary --port=5432 --username=replica_user --pgdata=/var/lib/postgresql/data --wal-method=stream --write-recovery-conf
      do
        echo 'Waiting for primary to be available...'
        sleep 5s
      done
      echo 'Backup done, starting replica...'
      postgres
      "
    depends_on:
      - postgres-primary
    networks:
      - game_network

volumes:
  redis_primary_data:
  redis_eu_data:
  postgres_primary_data:
  postgres_replica_data:

networks:
  game_network:
    driver: overlay
    attachable: true
```

### **AUTOMATED SCALING CONFIGURATION** üìà

```typescript
// Auto-scaling based on active sessions and performance metrics
export class DatabaseAutoScaler {
  private redis: Redis;
  private metrics: PrometheusRegistry;
  
  async monitorAndScale(): Promise<void> {
    const currentLoad = await this.getCurrentDatabaseLoad();
    const activeSessions = await this.getActiveSessionCount();
    
    // Scale Redis cluster based on memory usage and connection count
    if (currentLoad.redisMemoryUsage > 0.8 || currentLoad.redisConnections > 8000) {
      await this.scaleRedisCluster('up');
    } else if (currentLoad.redisMemoryUsage < 0.3 && currentLoad.redisConnections < 2000) {
      await this.scaleRedisCluster('down');
    }
    
    // Scale PostgreSQL read replicas based on query load
    if (currentLoad.postgresQueryLatency > 100 || currentLoad.postgresConnections > 150) {
      await this.scalePostgresReplicas('up');
    }
    
    // Automatic failover detection and recovery
    if (currentLoad.redisHealthStatus === 'unhealthy') {
      await this.initializeRedisFailover();
    }
  }
  
  private async scaleRedisCluster(direction: 'up' | 'down'): Promise<void> {
    const currentNodes = await this.redis.cluster('nodes');
    const nodeCount = currentNodes.split('\n').length - 1;
    
    if (direction === 'up' && nodeCount < 6) {
      // Add new Redis node to cluster
      await this.addRedisNode();
    } else if (direction === 'down' && nodeCount > 3) {
      // Remove Redis node from cluster
      await this.removeRedisNode();
    }
  }
}
```

---

This hybrid storage strategy provides the foundation for a 90% performance improvement while maintaining data consistency and enabling horizontal scaling as your tactical ASCII roguelike grows from 8 players to hundreds of concurrent players.